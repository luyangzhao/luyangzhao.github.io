---
---

@ARTICLE{zhaosci,
  author={Zhao, Luyang and Jiang, Yitao and Chen, Muhao and Bekris, Kostas and Balkcom, Devin},
  journal={Nature Communications}, 
  title={Modular shape-changing tensegrity-blocks enable self-assembling robotic structures}, 
  year={2025},
  abbr = {flexblocks},
  pdf = {papers/Tensegrity-Blocks.pdf},
  video = {https://drive.google.com/file/d/1uXKiN8vLOl9HqY2GBoFJa2bTcPd5-sz7/preview},
  link = {https://www.nature.com/articles/s41467-025-60982-0},
  abstract = {Modular robots are currently designed to perform a variety of tasks, primarily focusing on locomotion or manipulation through the reconfiguration of rigid modules. However, the potential to integrate multiple functions, such as making each robot deployable and capable of building lattice structures for self-construction and infrastructure creation, remains largely unexplored. To advance the field, we hypothesize that combining tensegrity principles with modular robotics can create lightweight, deformable units capable of integrating three critical functions within a single design: navigating varied terrains, manipulating arbitrary shape objects, and assembling weight-sustainable, active large infrastructures. Here, we designed untethered modular robots that are deformable, lightweight, deployable, outdoor-scale, capable of bearing loads, and capable of 3D attachment and detachment. With these characteristics, the system can form various 3D structures using different assembly methods, such as walking into position or being transported by rotorcraft. The deformability and lightweight nature of each block enable the assembled structures to dynamically change shape, providing new capabilities such as added compliance during locomotion and manipulation and the ability to interact with the environment in tasks like tent and bridge assemblies. In summary, we suggest that integrating lightweight and deformable properties into modular robot design offers potential improvements in their adaptability and multi-functionality.}}


@ARTICLE{starblocks,
  author={Zhao, Luyang and Wu, Yijia and Yan, Wenzhong and Zhan, Weishu and Huang, Xiaonan and Booth, Joran and Mehta, Ankur and Bekris, Kostas and Kramer-Bottiglio, Rebecca and Balkcom, Devin},
  journal={IEEE Robotics and Automation Letters}, 
  title={StarBlocks: Soft Actuated Self-Connecting Blocks for Building Deformable Lattice Structures}, 
  year={2023},
  volume={8},
  number={8},
  pages={4521-4528},
  doi={10.1109/LRA.2023.3284361},
  abstract = {In this paper, we present a soft modular block inspired by tensegrity structures that can form load-bearing structures through self-assembly. The block comprises a stellated compliant
skeleton, shape memory alloy muscles, and permanent magnet connectors. We classify five deformation primitives for individual blocks: bend, compress, stretch, stand, and shrink, which can be
combined across modules to reason about full-lattice deformation. Hierarchical function is abundant in nature and in human-designed
systems. Using multiple self-assembled lattices, we demonstrate the formation and actuation of 3-dimensional shapes, including a
load-bearing pop-up tent, a self-assembled wheel, a quadruped, a block-based robotic arm with gripper, and non-prehensile manipulation. To our knowledge, this is the first example of active deformable modules (blocks) that can reconfigure into different
load-bearing structures on-demand.},
  video={https://www.youtube.com/watch?v=xno0FBs3ZdQ},
  link={https://ieeexplore.ieee.org/abstract/document/10146508},
  pdf = {papers/soft2.pdf},
  abbr={soft2}
}

@ARTICLE{9738480,
  author={Zhao, Luyang and Wu, Yijia and Blanchet, Julien and Perroni-Scharf, Maxine and Huang, Xiaonan and Booth, Joran and Kramer-Bottiglio, Rebecca and Balkcom, Devin},
  journal={IEEE Robotics and Automation Letters}, 
  title={Soft Lattice Modules That Behave Independently and Collectively}, 
  year={2022},
  volume={7},
  number={3},
  pages={5942-5949},
  doi={10.1109/LRA.2022.3160611},
  arxiv={2110.11485},
  link={https://ieeexplore.ieee.org/abstract/document/9738480},
  video={https://www.youtube.com/watch?v=EYLYyijfzCg},
  pdf = {papers/soft1.pdf},
  abstract = {Natural systems integrate the work of many sub-units (cells) toward a large-scale unified goal (morphological and behav- ioral), which can counteract the effects of unexpected experiences, damage, or simply changes in tasks demands. In this letter, we exploit the opportunities presented by soft, modular, and tensegrity robots to introduce soft lattice modules that parallel the sub-units seen in biological systems. The soft lattice modules are comprised of 3D printed plastic “skeletons,” linear contracting shape mem- ory alloy spring actuators, and permanent magnets that enable adhesion between modules. The soft lattice modules are capable of independent locomotion, and can also join with other modules to achieve collective, self-assembled, larger scale tasks such as collective locomotion and moving an object across the surface of the lattice assembly. This work represents a preliminary step toward soft modular systems capable of independent and collective behaviors, and provide a platform for future studies on distributed control.},
  abbr = {soft1},}



@INPROCEEDINGS{airplane,
  author    = {Zhao, Luyang and Jiang, Yitao and She, Chun-Yi and Balkcom, Devin and Dong, Haibo and Chen, Muhao},
  title     = {Design and Experiment of a Lightweight Robotic Tensegrity Morphing Wing},
  booktitle = {AIAA SciTech Forum},
  year      = {2026},
  note      = {(accepted)},
  abbr      = {airplane}
}


@INPROCEEDINGS{9341312,
  author={Zhao, Luyang and Putman, Josiah and Wang, Weifu and Balkcom, Devin},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={PLRC*: A piecewise linear regression complex for approximating optimal robot motion}, 
  year={2020},
  volume={},
  number={},
  pages={6681-6688},
  doi={10.1109/IROS45743.2020.9341312},
  link={https://ieeexplore.ieee.org/abstract/document/9341312},
  pdf = {papers/plrc.pdf},
  abstract = {Discrete graphs are commonly used to approximately represent configuration spaces used in robot motion planning. This paper explores a representation in which the costs of crossing local regions of the configuration space are represented using piecewise linear regression (PLR). We explore a few simple motion planning problems, and show that for these problems, the memory required to store the representation compares favorably to that required for standard discrete vertex-and-edge models, while preserving the quality of paths returned from searches.},
  abbr={plrc}}

@INPROCEEDINGS{8901071,
  author={Putman, Josiah and Oh, Lisa and Zhao, Luyang and Honnold, Evan and Brown, Galen and Wang, Weifu and Balkcom, Devin},
  booktitle={International Symposium on Multi-Robot and Multi-Agent Systems (MRS)}, 
  title={LLDM: Locally linear distance maps for robot motion planning: Extended Abstract}, 
  year={2019},
  volume={},
  number={},
  pages={13-15},
  doi={10.1109/MRS.2019.8901071},
  pdf = {papers/lldm.pdf},
  link={https://ieeexplore.ieee.org/abstract/document/8901071},
  abstract={This paper presents a data structure that summarizes distances between configurations across a robot configuration space, using a binary space partition whose cells contain parameters used for a locally linear approximation of the distance function. Querying the data structure is extremely fast, particularly when compared to graph search required for querying Probabilistic Roadmaps, and memory requirements are promising. The paper explores the use of the data structure constructed for a single robot to provide a heuristic for challenging multi-robot motion planning problems. Potential applications also include the use of remote computation to analyze the space of robot motions, which then might be transmitted on-demand to robots with fewer computational resources.},
  abbr={lldm}}

@article{Putman2020PiecewiseLR,
  title={Piecewise linear regressions for approximating distance metrics},
  author={Josiah Putman and Lisa Oh and Luyang Zhao and Evan Honnold and Galen Brown and Weifu Wang and Devin J. Balkcom},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.12466},
  arxiv={2002.12466},
  link={https://api.semanticscholar.org/CorpusID:211572750},
  pdf = {papers/plr.pdf},
  abbr={plr}
}


@inproceedings{Ferland2018AssistiveAF,
  title={Assistive AI for Coping with Memory Loss},
  author={Libby Ferland and Ziwei Li and Shridhar Sukhani and Joan Zheng and Luyang Zhao and Maria L. Gini},
  booktitle={AAAI Workshops},
  year={2018},
  link={https://cdn.aaai.org/ocs/ws/ws0528/17360-76000-1-PB.pdf},
  pdf = {papers/assistant.pdf},
  abbr={assistant},
  abstract={Living with memory loss presents many challenges for patients and caregivers alike. Intelligent assistive technology can help address care gaps. Our core objective is to develop an assistive device that can be used at home by individuals experiencing memory impairment. We propose an assistant, based on existing and widely used voice activated consumer technology, as a tool to help patients and caregivers cope with issues common to cognitive impairment. Our long term objective is to develop a cognitive assistant that can do what is told to do, but also reason and be proactive in interacting with its users.}
}

@inproceedings{jeong2024multimodal,
  title={Multi-modal Perception Dataset of In-water Objects for Autonomous Surface Vehicles}, 
  author={Mingi Jeong and Arihant Chadda and Ziang Ren and Luyang Zhao and Haowen Liu and Monika Roznere and Aiwei Zhang and Yitao Jiang and Sabriel Achong and Samuel Lensgraf and Alberto Quattrini Li},
  year={2024},
      arxiv={2404.18411},
  abstract={This paper introduces the first publicly accessible labeled multi-modal perception dataset for autonomous maritime navigation, focusing on in-water obstacles within the aquatic environment to enhance situational awareness for Autonomous Surface Vehicles (ASVs). This dataset, collected over 4 years and consisting of diverse objects encountered under varying environmental conditions, aims to bridge the research gap in autonomous surface vehicles by providing a multi-modal, annotated, and ego-centric perception dataset, for object detection and classification. We also show the applicability of the proposed dataset by training deep learning-based open-source perception algorithms that have shown success. We expect that our dataset will contribute to development of the marine autonomy pipelines and marine (field) robotics. This dataset is opensource and can be found at this https URL.},
booktitle = { IEEE ICRA Workshop on Field Robotics},
   abbr = {seesea}   
}



@ARTICLE{surface,
  author={Luyang Zhao and Yitao Jiang and Chun-Yi She and Alberto Quattrini Li and Muhao Chen and Devin Balkcom},
  journal={npj Robotics (under revision)}, 
  title={SoftRafts: Floating and Adaptive Soft Modular Robots}, 
  pdf = {papers/SoftRaft.pdf},
  abstract = {Modular robots possess great potential due to their adaptability and reconfigurability, yet their use in aquatic environments and dynamic multi-tasking scenarios—particularly for complex manipulation—remains largely underexplored. To address the need for versatile and multifunctional systems in such settings, we hypothesize that integrating soft-bending capabilities into modular robots can create a platform capable of navigating complex environments, performing diverse manipulation tasks, and assembling deformable lattices. In this work, we present a variable-stiffness soft modular robot that combines rigid 3D printed components with soft foam, utilizing a cable-actuated mechanism and a propeller. This modular robot can locomote, bend, steer, connect with other modules, and assemble into various larger active structures for different applications. For instance, when configured as a gripper, the robot can collect trash from the water's surface. When assembled into a raft, it functions as a movable platform for drone landings. In a chain configuration, the robot moves like a snake on land and transitions seamlessly to aquatic locomotion using a propeller. Additionally, these robots can operate collectively like swarm robots, such as transporting boxes collaboratively across surfaces. Our findings highlight that incorporating deformable features into modular robot designs significantly enhances their adaptability and multifunctionality in aquatic environments.},
  video = {https://drive.google.com/file/d/1fY3anskQE_okj0axE6tPutMyHkgNZe5w/preview},
  year={2025},
  code={https://github.com/luyangzhao/SoftRafts},
  abbr = {surface}}





@ARTICLE{softsnap,
      title={SoftSnap: Rapid Prototyping of Untethered Soft Robots Using Snap-Together Modules}, 
      author={Luyang Zhao and Yitao Jiang and Chun-Yi She and Muhao Chen and Devin Balkcom},
      journal={ Soft Robotics}, 
      year={2025},
      code={https://github.com/luyangzhao/SoftSnap.git},
      video={https://www.youtube.com/watch?v=SWuYzo7xSno},
      link={https://www.liebertpub.com/doi/10.1089/soro.2024.0170}, 
      pdf = {papers/softsnap.pdf},
      doi = {10.1089/soro.2024.0170},
      arxiv={2410.19169},
      abstract = {Soft robots offer adaptability and safe interaction with complex environments.
Rapid prototyping kits that allow soft robots to be assembled easily will allow
different geometries to be explored quickly to suit different environments or to
mimic the motion of biological organisms. We introduce SoftSnap modules: snaptogether components that enable the rapid assembly of a class of untethered soft
robots. Each SoftSnap module includes embedded computation, motor-driven
string actuation, and a flexible thermoplastic polyurethane (TPU) printed structure capable of deforming into various shapes based on the string configuration.
These modules can be easily connected with other SoftSnap modules or customizable connectors. We demonstrate the versatility of the SoftSnap system through
four configurations: a starfish-like robot, a brittle star robot, a snake robot, a
3D gripper, and a ring-shaped robot. These configurations highlight the ease of
assembly, adaptability, and functional diversity of the SoftSnap modules. The
SoftSnap modular system offers a scalable, snap-together approach to simplifying soft robot prototyping, making it easier for researchers to explore untethered
soft robotic systems rapidly.},
    abbr = {softsnap},
}


@ARTICLE{dolphin,
      title={An Untethered Bioinspired Robotic Tensegrity Dolphin with
Multi-Flexibility Design for Aquatic Locomotion}, 
      author={Luyang Zhao and Yitao Jiang and Chun-Yi She and Mingi Jeong and Haibo Dong and Alberto Quattrini Li and Muhao Chen and Devin Balkcom},
      journal={RoboSoft}, 
      year={2025},
      eprint={2410.19169},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      video={https://www.youtube.com/watch?v=avUUYTJ178g},
      link={https://ieeexplore.ieee.org/abstract/document/11020959}, 
      arxiv={2411.00347},
      pdf = {papers/dolphin.pdf},
      abstract = {This paper presents the first steps toward a soft
dolphin robot using a bio-inspired approach to mimic dolphin
flexibility. The current dolphin robot uses a minimalist approach, with only two actuated cable-driven degrees of freedom
actuated by a pair of motors. The actuated tail moves up and
down in a swimming motion, but this first proof of concept does
not permit controlled turns of the robot. While existing robotic
dolphins typically use revolute joints to articulate rigid bodies,
our design – which will be made opensource – incorporates a
flexible tail with tunable silicone skin and actuation flexibility
via a cable-driven system, which mimics muscle dynamics and
design flexibility with a tunable skeleton structure. The design is
also tunable since the backbone can be easily printed in various
geometries. The paper provides insights into how a few such
variations affect robot motion and efficiency, measured by speed
and cost of transport (COT). This approach demonstrates the
potential of achieving dolphin-like motion through enhanced
flexibility in bio-inspired robotics.},
    abbr = {dolphin},
}


@ARTICLE{learning,
      title={On the Exploration of LM-Based Soft Modular Robot Design}, 
      author={Weicheng Ma* and Luyang Zhao* and Chun-Yi She* and Yitao Jiang and Alan Sun and Bo Zhu and Devin Balkcom and Soroush Vosoughi},
      journal={ * means equal contribution}, 
      eprint={2410.19169},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/pdf/2411.00345}, 
      pdf = {papers/learning.pdf},
      arxiv = {2411.00345},
      year={2024},
      abstract = {Recent large language models (LLMs) have demonstrated
promising capabilities in modeling real-world knowledge and
enhancing knowledge-based generation tasks. In this paper,
we further explore the potential of using LLMs to aid in
the design of soft modular robots, taking into account both
user instructions and physical laws, to reduce the reliance
on extensive trial-and-error experiments typically needed to
achieve robot designs that meet specific structural or task
requirements. Specifically, we formulate the robot design
process as a sequence generation task and find that LLMs
are able to capture key requirements expressed in natural
language and reflect them in the construction sequences
of robots. To simplify, rather than conducting real-world
experiments to assess design quality, we utilize a simulation
tool to provide feedback to the generative model, allowing
for iterative improvements without requiring extensive human annotations. Furthermore, we introduce five evaluation
metrics to assess the quality of robot designs from multiple
angles including task completion and adherence to instructions, supporting an automatic evaluation process. Our model
performs well in evaluations for designing soft modular
robots with uni- and bi-directional locomotion and stairdescending capabilities, highlighting the potential of using
natural language and LLMs for robot design. However, we
also observe certain limitations that suggest areas for further
improvement.},
    abbr = {learning},
}




@ARTICLE{llmswarm,
      title={Exploring Spontaneous Social Interaction Swarm Robotics Powered by Large Language Models},
      author={Yitao Jiang and Luyang Zhao and Alberto Quattrini Li and Muhao Chen and Devin Balkcom},
      journal={IROS 2025},
      url={https://www.researchgate.net/profile/Yitao-Jiang/publication/389490353_Exploring_Spontaneous_Social_Interaction_Swarm_Robotics_Powered_by_Large_Language_Models/links/67c4066af5cb8f70d5c49e37/Exploring-Spontaneous-Social-Interaction-Swarm-Robotics-Powered-by-Large-Language-Models.pdf},
      pdf={papers/llmswarm.pdf},
      abstract={Traditional swarm robots rely on specific communication and planning strategies to coordinate particular tasks. Human swarms exhibit distinctive characteristics due to their capacity for language-based communication and active reasoning. This paper presents an exploratory approach to robotic swarm intelligence that leverages Large Language Models (LLMs) to emulate human-like active problem-solving behaviors. We introduce a decentralized multi-robot system where each robot initially only has its local information and does not know others’ existence. The robots utilize LLMs for reasoning and natural language for inter-robot communication, enabling them to discover peers, share information, and coordinate actions dynamically. In a series of experiments in zero-shot settings, we observed human-like social behaviors, including mutual discovery, identification, information exchange, collaboration, negotiation, and error correction. While the technical approach is straightforward, the main contribution lies in exploring the interactive societies that LLM-driven robots form—a form of “robot anthropology” that examines emergent collaborative structures.},
      video = 
      {https://drive.google.com/file/d/14bz-UbZI_IxojBwJtlQ1MuGJati8-cCm/preview},
      abbr={llmswarm},
}


@ARTICLE{seepersea,
  title   = {SeePerSea: Multi-modal Perception Dataset of In-water Objects for Autonomous Surface Vehicles},
  author  = {Jeong, Mingi and Chadda, Arihant and Ren, Ziang and Zhao, Luyang and Liu, Haowen and Roznere, Monika and Zhang, Aiwei and Jiang, Yitao and Achong, Sabriel and Lensgraf, Samuel and Quattrini Li, Alberto},
  journal = {IEEE Transactions on Field Robotics},
  year    = {2025},
  doi     = {10.1109/TFR.2025.3602937},
  link     = {https://doi.org/10.1109/TFR.2025.3602937},
  pdf     = {papers/seepersea.pdf},
  abbr    = {seepersea},
  note    = {(accepted)},
  abstract = {This paper introduces the first publicly accessible labeled multi-modal perception dataset for autonomous maritime navigation, focusing on in-water obstacles within the aquatic environment to enhance situational awareness for Autonomous Surface Vehicles (ASVs). This dataset, collected over 4 years and consisting of diverse objects encountered under varying environmental conditions, aims to bridge the research gap in autonomous surface vehicles by providing a multimodal, annotated, and ego-centric perception dataset, for object detection and classification. We also show the applicability of the proposed dataset by training deep learning-based open-source perception algorithms that have shown success. We expect that our dataset will contribute to development of the marine autonomy pipelines and marine (field) robotics. This dataset is open source and can be found at https://seepersea.github.io/.}
}




